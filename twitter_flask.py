# -*- coding: utf-8 -*-
"""Twitter_Flask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DSssrm1RKLhWfuSV-cD8ztWeLmJTPugy
"""

!pip install fuzzymatcher
!pip install tweepy
!pip install pycountry
!pip install nltk
!pip install textblob
!pip install wordcloud
!pip install scikit-learn
!pip install pickle-mixin
!pip install pycountry

import os
import tweepy as tw
import pandas as pd
import time
import sys
import datetime 
from pathlib import Path
import fuzzymatcher
from google.colab import files
import pickle
import nltk
import re
import pycountry

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer


# Import word_tokenize and stopwords from nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from textblob import TextBlob


#key and token generated through Twitter Developer module
consumer_key= 'QLdhEgbqKEZCSVsW0kedh4cXi'
consumer_secret= '1SwSssnHayK8c1bdsiDksMhvaztv9rDQ4cva2NM7VTafXQb9Yr'
access_token= '1441295872852066306-mINc6c8y3TwGaUEV8ytUdqkipK6lua'
access_token_secret= 'W6oC0kDfLjucXVPDJm8ZTtrQPxgYqmw5G922SS0SEGGBg'

def get_tweets(search_words):
    auth = tw.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    api = tw.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
    
    
    start_date="2021-01-01"


    # Collect tweets
    tweets = tw.Cursor(api.search,
                           q=search_words,
                           lang="en",
                           since=start_date).items(2000) 
    
    # Collect a list of tweets
    users_locs = [[tweet.user.screen_name, tweet.user.location,tweet.text,tweet.created_at, tweet.id, tweet.id_str, tweet.truncated, tweet.entities, tweet.source, tweet.in_reply_to_status_id, tweet.in_reply_to_status_id_str, tweet.in_reply_to_user_id, tweet.in_reply_to_user_id_str, tweet.in_reply_to_screen_name, tweet.user, tweet.geo,tweet.coordinates, tweet.place, tweet.contributors,tweet.is_quote_status, tweet.retweet_count, tweet.favorite_count, tweet.favorited, tweet.retweeted, tweet.lang] for tweet in tweets]
    users_locs

            
    #making dataframe for the list
    tweet_text = pd.DataFrame(data=users_locs, 
                                columns=[ "user.screen_name",  "user.location", "text", "created_at",  "id",  "id_str",  
                                         "truncated",  "entities",  "source",  "in_reply_to_status_id",  "in_reply_to_status_id_str", 
                                         "in_reply_to_user_id",  "in_reply_to_user_id_str",  "in_reply_to_screen_name", 
                                         "user",  "geo", "coordinates",  "place",  "contributors", "is_quote_status",  
                                         "retweet_count",  "favorite_count",  "favorited",  "retweeted",  "lang"])
    return tweet_text

hashtag=input("Enter the hashtag (eg : #wellbeing) =")
WHITE=get_tweets(str(hashtag))
WHITE

# We do not need first two columns. Let's drop them out.
WHITE.drop(columns=["coordinates","place","contributors"], axis=1, inplace=True)
# Drop duplicated rows
WHITE["created_at"] = pd.to_datetime(WHITE["created_at"])
# Print the info again
print(WHITE.info()) #wellbeing

def process_tweets(tweet):
    
    #remove emoji
    tweet =re.sub( "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+",'', str(tweet),flags=re.MULTILINE) # no emoji
    
    
    # Remove links
    tweet = re.sub(r"http\S+", "", tweet,flags=re.MULTILINE)
    
    # Remove mentions and hashtag
    tweet = re.sub(r'\@\w+|\#','', tweet)
    
    
    
    # Tokenize the words
    tokenized = word_tokenize(tweet)

    # Remove the stop words
    tokenized = [token for token in tokenized if token not in stopwords.words("english")] 

    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    tokenized = [lemmatizer.lemmatize(token, pos='a') for token in tokenized]

    # Remove non-alphabetic characters and keep the words contains three or more letters
    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]
    
    return tokenized
    
# Call the function and store the result into a new column
WHITE["Processed"] = WHITE["text"].str.lower().apply(process_tweets)

# Print the first fifteen rows of Processed
display(WHITE[["Processed"]])



# Get the tweet lengths
WHITE["Length"] = WHITE["text"].str.len()
# Get the number of words in tweets
WHITE["Words"] = WHITE["text"].str.split().str.len()
# Display the new columns
display(WHITE[["Length", "Words"]])


# Fill the missing values with unknown tag
WHITE["user.location"].fillna("unknown", inplace=True)
# Print the unique locations and number of unique locations
"Unique Values:",WHITE["user.location"].unique()
print("Unique Value count:",len(WHITE["user.location"].unique()))

def get_countries(location):
    
    # If location is a country name return its alpha2 code
    if pycountry.countries.get(name= location):
        return pycountry.countries.get(name = location).alpha_2
    
    # If location is a subdivisions name return the countries alpha2 code
    try:
        pycountry.subdivisions.lookup(location)
        return pycountry.subdivisions.lookup(location).country_code
    except:
        # If the location is neither country nor subdivision return the "unknown" tag
        return "unknown"

# Call the function and store the country codes in the Country column
WHITE["Country"] = WHITE["user.location"].apply(get_countries)

# Print the unique values
print(WHITE["Country"].unique())

# Print the number of unique values
print("Number of unique values:",len(WHITE["Country"].unique()))

type(WHITE["Processed"])
flat_list = [item for sublist in WHITE["Processed"] for item in sublist]



# Create our contextual stop words
tfidf_stops =["cash"
,"compensation"
,"earnings"
,"interest"
,"livelihood"
,"pay"
,"proceeds"
,"profit"
,"revenue"
,"royalty"
,"salary"
,"wage"
,"assets"
,"avails"
,"benefits"
,"commission"
,"dividends"
,"drawings"
,"gains"
,"gravy"
,"gross"
,"harvest"
,"honorarium"
,"means"
,"payoff"
,"receipts"
,"returns"]




# Initialize a Tf-idf Vectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words= tfidf_stops)
# Fit and transform the vectorizer
tfidf_matrix = vectorizer.fit_transform(flat_list)
# Let's see what we have
display(tfidf_matrix)
# Create a DataFrame for tf-idf vectors and display the first rows
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns= vectorizer.get_feature_names())
tweets_processed=tfidf_df.head()
display(tweets_processed)

tweets_processed=WHITE


print("Since:",tweets_processed["created_at"].min())
# Print the maximum datetime
print("Until",tweets_processed["created_at"].max())


# Print the value counts of Country column
#print(tweets_processed["Country"].value_counts())
# Create a new DataFrame called frequencies
frequencies = pd.DataFrame(tfidf_matrix.sum(axis=0).T,index=vectorizer.get_feature_names(),columns=['total frequency'])
# Display the most 20 frequent words
#display(frequencies.sort_values(by='total frequency',ascending=False))


# Add polarities and subkectivities into the DataFrame by using TextBlob
tweets_processed["Polarity"] = tweets_processed["Processed"].apply(lambda word: TextBlob(str(word)).sentiment.polarity)
tweets_processed["Subjectivity"] = tweets_processed["Processed"].apply(lambda word: TextBlob(str(word)).sentiment.subjectivity)

# Display the Polarity and Subjectivity columns
#display(tweets_processed[["Polarity","Subjectivity"]])

# Define a function to classify polarities
def analyse_polarity(polarity):
    if polarity > 0:
        return "Positive"
    if polarity == 0:
        return "Neutral"
    if polarity < 0:
        return "Negative"

# Apply the funtion on Polarity column and add the results into a new column
tweets_processed["Polarity Scores"] = tweets_processed["Polarity"].apply(analyse_polarity)

# Display the Polarity and Subjectivity Analysis
#display(tweets_processed[["Polarity Scores"]])

# Print the value counts of the Label column
print(tweets_processed["Polarity Scores"].value_counts())
tweets_processed